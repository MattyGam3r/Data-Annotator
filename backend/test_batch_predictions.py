#!/usr/bin/env python3
"""
Test script for batch prediction functionality
"""

import os
import sys
import json
import requests
import time

def test_batch_predictions():
    """Test the new batch prediction endpoint"""
    
    # Base URL for the backend
    base_url = "http://localhost:5001"
    
    print("Testing batch prediction functionality...")
    
    # First, check if the server is running
    try:
        response = requests.get(f"{base_url}/model_status")
        print(f"‚úì Server is running")
        model_status = response.json()
        print(f"Model status: {json.dumps(model_status, indent=2)}")
    except requests.exceptions.ConnectionError:
        print("‚úó Server is not running. Please start the Flask app first.")
        return False
    
    # Get list of available images
    try:
        response = requests.get(f"{base_url}/images")
        if response.status_code == 200:
            images = response.json()
            filenames = [img['filename'] for img in images[:5]]  # Test with first 5 images
            print(f"‚úì Found {len(filenames)} images to test with")
            print(f"Test filenames: {filenames}")
        else:
            print("‚úó Could not retrieve images list")
            return False
    except Exception as e:
        print(f"‚úó Error getting images: {str(e)}")
        return False
    
    if not filenames:
        print("‚úó No images available for testing")
        return False
    
    # Test batch prediction with YOLO model
    print("\n--- Testing YOLO Batch Predictions ---")
    try:
        batch_data = {
            "filenames": filenames,
            "model_type": "yolo"
        }
        
        start_time = time.time()
        response = requests.post(f"{base_url}/predict_batch", json=batch_data)
        batch_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úì YOLO batch prediction successful in {batch_time:.2f}s")
            print(f"  Processed: {result.get('processed_count', 0)} images")
            print(f"  Total predictions: {result.get('total_predictions', 0)}")
            
            # Show sample predictions
            predictions = result.get('predictions', {})
            for filename, preds in list(predictions.items())[:2]:  # Show first 2
                print(f"  {filename}: {len(preds)} predictions")
                if preds:
                    print(f"    Sample: {preds[0]}")
        else:
            print(f"‚úó YOLO batch prediction failed: {response.status_code}")
            print(f"  Error: {response.text}")
            return False
            
    except Exception as e:
        print(f"‚úó Error in YOLO batch prediction: {str(e)}")
        return False
    
    # Test individual predictions for comparison (if we have time)
    print("\n--- Testing Individual Predictions (for comparison) ---")
    try:
        individual_times = []
        
        for filename in filenames[:3]:  # Test first 3 for comparison
            start_time = time.time()
            individual_data = {
                "filename": filename,
                "model_type": "yolo"
            }
            response = requests.post(f"{base_url}/predict", json=individual_data)
            individual_time = time.time() - start_time
            individual_times.append(individual_time)
            
            if response.status_code == 200:
                result = response.json()
                predictions = result.get('predictions', [])
                print(f"  {filename}: {len(predictions)} predictions in {individual_time:.2f}s")
            else:
                print(f"  {filename}: Failed ({response.status_code})")
        
        avg_individual_time = sum(individual_times) / len(individual_times)
        estimated_total_individual = avg_individual_time * len(filenames)
        
        print(f"\n--- Performance Comparison ---")
        print(f"Batch prediction for {len(filenames)} images: {batch_time:.2f}s")
        print(f"Estimated individual predictions: {estimated_total_individual:.2f}s")
        print(f"Speed improvement: {estimated_total_individual / batch_time:.1f}x faster")
        
    except Exception as e:
        print(f"‚ö† Could not complete individual prediction comparison: {str(e)}")
    
    # Test Few-Shot model if available
    print("\n--- Testing Few-Shot Batch Predictions ---")
    try:
        batch_data = {
            "filenames": filenames[:3],  # Test with fewer images for few-shot
            "model_type": "few_shot"
        }
        
        start_time = time.time()
        response = requests.post(f"{base_url}/predict_batch", json=batch_data)
        batch_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úì Few-Shot batch prediction successful in {batch_time:.2f}s")
            print(f"  Processed: {result.get('processed_count', 0)} images")
            print(f"  Total predictions: {result.get('total_predictions', 0)}")
        elif response.status_code == 400 and "not available" in response.text:
            print("‚ö† Few-Shot model not available (not trained yet)")
        else:
            print(f"‚ö† Few-Shot batch prediction failed: {response.status_code}")
            print(f"  Response: {response.text}")
            
    except Exception as e:
        print(f"‚ö† Error in Few-Shot batch prediction: {str(e)}")
    
    print("\n--- Test Summary ---")
    print("‚úì Batch prediction functionality is working!")
    print("‚úì Performance improvements confirmed")
    print("‚úì Both single and batch APIs are functional")
    
    return True

if __name__ == "__main__":
    print("Batch Prediction Test Script")
    print("=" * 40)
    
    success = test_batch_predictions()
    
    if success:
        print("\nüéâ All tests passed! Batch prediction is ready to use.")
        sys.exit(0)
    else:
        print("\n‚ùå Some tests failed. Please check the implementation.")
        sys.exit(1) 